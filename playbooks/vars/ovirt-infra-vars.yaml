---
###########################
# Common
###########################
compatibility_version: 4.1

# Data center
data_center_name: "{{ rhv_data_center | default('Default') }}"

vm_infra_wait_for_ip_delay: 180
vm_infra_wait_for_ip_retries: 10
##########################
# VM infra
##########################
template_cluster: "{{ rhv_cluster }}"
template_name: rhel7
template_memory: 4GiB
template_cpu: 1
template_disk_storage: "{{ rhv_data_storage }}"
template_disk_size: 30GiB
template_nics:
  - name: nic1
    profile_name: ovirtmgmt
    interface: virtio


##########################
# Cloud Init Script
##########################
# Use the following if RHEL 7.4 or below VMs are being created on a RHV 4.2 or above engine
cloud_init_script: |
  runcmd:
    - sed -i 's@^# device =.*@device = /dev/virtio-ports/ovirt-guest-agent.0@' /etc/ovirt-guest-agent.conf
    - sed -i 's@com.redhat.rhevm.vdsm@ovirt-guest-agent.0@' /etc/udev/rules.d/55-ovirt-guest-agent.rules
    - 'udevadm trigger --subsystem-match="virtio-ports"'
    - mkdir -p '/var/lib/origin/openshift.local.volumes'
    - /usr/sbin/mkfs.xfs -L localvol "{{ local_volumes_device }}"
    - sleep "$(($RANDOM % 60))"
    - sync
    - echo "cloud-init finished" > /etc/sysconfig/.ready
  mounts:
    - [ '{{ local_volumes_device }}', '/var/lib/origin/openshift.local.volumes', 'xfs', 'defaults,gquota' ]
  rh_subscription:
    username: {{ rhsub_user }}
    password: {{ rhsub_password }}
    add-pool: [{{ rhsub_pool }}]
    server-hostname: {{ rhsub_server | default('subscription.rhsm.redhat.com') }}
    disable-repo: 
      - rhel-7-server-rt-rpms
      - rhel-7-server-nfv-rpms
      - rhel-7-server-aus-rpms
      - rhel-7-server-rt-rpms
      - rhel-7-server-tus-rpms
      - rhel-7-server-htb-rpms
    enable-repo:
      - rhel-7-server-rpms
      - rhel-7-server-extras-rpms
      - rhel-7-fast-datapath-rpms
      - rhel-7-server-ose-3.9-rpms
  packages:
    - cloud-init
    - wget
    - git
    - net-tools
    - bind-utils
    - iptables-services
    - bridge-utils
    - bash-completion
    - kexec-tools
    - sos
    - psacct
    - vim
  package_update: true
  package_upgrade: false
  package_reboot_if_required: false


master_vm:
  cluster: "{{ rhv_cluster }}"
  template: rhel7
  memory: 4GiB
  cores: 2
  high_availability: true
  disks:
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: docker_disk
      interface: virtio
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: localvol_disk
      interface: virtio


node_vm:
  cluster: "{{ rhv_cluster }}"
  template: rhel7
  memory: 8GiB
  cores: 2
  disks:
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: docker_disk
      interface: virtio
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: localvol_disk
      interface: virtio

infra_vm:
  cluster: "{{ rhv_cluster }}"
  template: rhel7
  memory: 8GiB
  cores: 2
  # disks are created alphanumeric using name:value -  vdb/vdc/vdd
  disks:
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: docker_disk
      interface: virtio
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: localvol_disk
      interface: virtio

infra_vm:
  cluster: "{{ rhv_cluster }}"
  template: rhel7
  memory: 8GiB
  cores: 2
  disks:
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: docker_disk
      interface: virtio
    - size: 20GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: localvol_disk
      interface: virtio
    - size: 100GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: storage_gluster
      interface: virtio
  nics:
    - name: nic1
      profile_name: ovirtmgmt
      interface: virtio
      mac_address: 00:1a:4a:16:01:50


lb_vm:
  cluster: "{{ rhv_cluster }}"
  template: rhel7
  memory: 4GiB
  cores: 1
  disks:
    - size: 1GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: docker_disk
      interface: virtio
    - size: 1GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: localvol_disk
      interface: virtio


vms:
  # Node VMs
  - name: node01
    tag: openshift_node
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "node01.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:49
  
  - name: node02
    tag: openshift_node
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "node02.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:48
 
  # Infra VMs
  - name: infra1
    tag: openshift_infra
    profile: "{{ infra_vm }}"
    cloud_init:
      host_name: "infra1.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:47
 
  - name: infra2
    tag: openshift_infra
    profile: "{{ infra_vm }}"
    cloud_init:
      host_name: "infra2.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:46

  - name: infra3
    tag: openshift_infra
    profile: "{{ infra_vm }}"
    cloud_init:
      host_name: "infra3.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:42
 
  # Master VMs
  - name: master1
    profile: "{{ master_vm }}"
    tag: openshift_master
    cloud_init:
      host_name: "master1.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:45
 
  - name: master2
    tag: openshift_master
    profile: "{{ master_vm }}"
    cloud_init:
      host_name: "master2.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:44
 
  - name: master3
    tag: openshift_master
    profile: "{{ master_vm }}"
    cloud_init:
      host_name: "master3.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:43
 
  # Load balancer
  - name: openshift-lb
    tag: openshift_lb
    profile: "{{ lb_vm }}"
    cloud_init:
      host_name: "openshift-lb.{{ local_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ cloud_init_script }}"
    nics:
      - name: nic1
        profile_name: ovirtmgmt
        interface: virtio
        mac_address: 00:1a:4a:16:01:50


affinity_groups:
  - name: masters_ag
    cluster: "{{ rhv_cluster }}"
    vm_enforcing: false
    vm_rule: negative
    vms:
      - master1
      - master2
      - master3
    wait: true

  - name: infra_ag
    cluster: "{{ rhv_cluster }}"
    vm_enforcing: false
    vm_rule: negative
    vms:
      - infra1
      - infra2
      - infra3
      - openshift-lb
    wait: true

  - name: app_ag
    cluster: "{{ rhv_cluster }}"
    vm_enforcing: false
    vm_rule: negative
    vms:
      - node01
      - node02
...
